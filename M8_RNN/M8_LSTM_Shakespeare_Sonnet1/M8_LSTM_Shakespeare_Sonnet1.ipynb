{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256484e8-5f10-45f5-9bfe-0f9a4e14d00b",
   "metadata": {},
   "source": [
    "# Shakespeare next word prediction\n",
    "- #### An RNN is used to predict the next word based on the previous word(s).\n",
    "- #### Parts from a Shakespeare poem (Shakespeareâ€™s first sonnet) are used to train the network\n",
    "### **Network Architecture**\n",
    "1. The input layer is an embedding layer.\n",
    "2. The first hidden layer is an LSTM layer.\n",
    "3. The second hidden layer is a dropout layer.\n",
    "4. The output layer is a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8129a3bc-21a2-4e74-9d9b-ad4d70489a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HFO\\.conda\\envs\\tf\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e03438-70a0-43ea-a70b-fbe79f7f75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Shakespeare_Sonnet_1.txt') as file: \n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f696b373-cd1d-43f3-893e-3108349b154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the text file as separate lines of text\n",
    "with open('Shakespeare_Sonnet_1.txt') as file: \n",
    "    text = file.read()\n",
    "    lines = text.lower().split('\\n')\n",
    "\n",
    "#print(lines)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38890dc-211f-48e7-99b5-b35d47f7de24",
   "metadata": {},
   "source": [
    "# By default, the text_to_word_sequence function:\n",
    "- Removes all punctuation including tabs and newlines.\n",
    "- Converts all words to lower case.\n",
    "- Splits the input string into words using space as the separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c1c5c0-008d-4eef-93e1-54b2a3282488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define words, vocabulary size and sequences of words as lines\n",
    "words = text_to_word_sequence(text)\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89a299-42a2-468c-8ef3-74a312de538d",
   "metadata": {},
   "source": [
    "# Use a Tokenizer from the keras.preprocessing.text library to convert the input text into indexed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e208fe5-6a3b-43d6-b3b7-f0ff555d59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(words)\n",
    "tokens = tokenizer.word_index\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee85aa02-cb4e-4e96-b41c-a07941dd607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580\n",
      "580\n"
     ]
    }
   ],
   "source": [
    "#tokenizer.word_index\n",
    "print(len(np.unique(words)))\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561f8a2-a10f-4322-a021-5f8ce10ad211",
   "metadata": {},
   "source": [
    "# Get the Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32991e9c-f3ec-4702-a8b0-0e34cce196ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0b960-b017-489d-a962-a6e25ade157c",
   "metadata": {},
   "source": [
    "# Build Sequences\n",
    "Use the texts_to_sequences() method to convert lines into sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4678bccd-7d8d-4811-813f-9c7c1c0b7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build subsequences of different sizes, starting from 1 to the size of the sequence. \n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "subsequences = []\n",
    "for sequence in sequences:\n",
    "    for i in range(1, len(sequence)):\n",
    "       subsequence = sequence[:i+1]\n",
    "       subsequences.append(subsequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5d9eb-9995-466f-b425-0a6883681b08",
   "metadata": {},
   "source": [
    "### Use the pad_sequences from the keras.preprocessing.sequences library to pad the subsequences with zeros so that all sequences are made the same size.\n",
    "### The padding='pre' parameter tells the function to add padding at the beginning of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c48b2b1-2321-4067-bef7-903c6f8a4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequences = []\n",
    "for sequence in sequences:\n",
    "    for i in range(1, len(sequence)):\n",
    "        subsequence = sequence[:i+1]\n",
    "        subsequences.append(subsequence)\n",
    "        \n",
    "sequence_length = max([len(sequence) for sequence in sequences])\n",
    "sequences = pad_sequences(subsequences, maxlen=sequence_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8c1b5-7acd-41ad-aaac-1f862f4947d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe410d90-d430-497d-a7c0-f4ac712b0080",
   "metadata": {},
   "source": [
    "# Build Input and Output\n",
    "#### The input to the network is all the words of the sequence except the last one, and the output is the last word of the sequence.\n",
    "#### Then, convert output into categorical data using the to_categorical() method from the keras.utils module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c97eadb-52bd-42f8-be93-2df6caf72f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402c02f7-6c0c-43e6-a8c9-c593cf87847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1177, 9)\n",
      "(1177, 581)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48810937-60c5-4514-9ccd-b7b1d7448494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4129dfcf-f67f-406a-a8fe-4a7a0d153d02",
   "metadata": {},
   "source": [
    "# Train an LSTM to perform NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52d3fe0-0fd0-4e09-baf0-23e3c44e18fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HFO\\.conda\\envs\\tf\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, 100, input_length=sequence_length - 1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9ca48c4-62eb-4096-a9d6-488ecf1ba2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HFO\\.conda\\envs\\tf\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b23bad-529b-429f-ace4-e010dfb456be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1999a-5592-4c45-ad79-87acb3e4e7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "706d6843-c015-4a5b-94be-408a21d53f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffe49111-18eb-401f-9f50-364167c38d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2622 - accuracy: 0.9303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2621830105781555, 0.9303313493728638]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca629f7-c0b6-4a93-9ccb-45c87e1aa2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03a7965-0c41-4aed-b242-192f6b0644c8",
   "metadata": {},
   "source": [
    "# Show 10 single predictions using the different sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8821828d-46b0-47f9-9b74-cd9b13157efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.Input Sequence-tokens:  [  0   0   0   0   0   0   0   0  26 189]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 0, 0, 0, 0, 26]\n",
      "\t\tPredicted Word-token:  [189]\n",
      "0.Input Sequence-text:  ['from fairest']\n",
      "\tInput Word-text:  ['from']\n",
      "\t\tPredicted Word-text:  ['fairest']\n",
      "\n",
      "\n",
      "1.Input Sequence-tokens:  [  0   0   0   0   0   0   0  26 189 190]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 0, 0, 0, 26, 189]\n",
      "\t\tPredicted Word-token:  [190]\n",
      "1.Input Sequence-text:  ['from fairest creatures']\n",
      "\tInput Word-text:  ['from fairest']\n",
      "\t\tPredicted Word-text:  ['creatures']\n",
      "\n",
      "\n",
      "2.Input Sequence-tokens:  [  0   0   0   0   0   0  26 189 190 191]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 0, 0, 26, 189, 190]\n",
      "\t\tPredicted Word-token:  [191]\n",
      "2.Input Sequence-text:  ['from fairest creatures we']\n",
      "\tInput Word-text:  ['from fairest creatures']\n",
      "\t\tPredicted Word-text:  ['we']\n",
      "\n",
      "\n",
      "3.Input Sequence-tokens:  [  0   0   0   0   0  26 189 190 191  97]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 0, 26, 189, 190, 191]\n",
      "\t\tPredicted Word-token:  [97]\n",
      "3.Input Sequence-text:  ['from fairest creatures we desire']\n",
      "\tInput Word-text:  ['from fairest creatures we']\n",
      "\t\tPredicted Word-text:  ['desire']\n",
      "\n",
      "\n",
      "4.Input Sequence-tokens:  [  0   0   0   0  26 189 190 191  97  98]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 26, 189, 190, 191, 97]\n",
      "\t\tPredicted Word-token:  [98]\n",
      "4.Input Sequence-text:  ['from fairest creatures we desire increase']\n",
      "\tInput Word-text:  ['from fairest creatures we desire']\n",
      "\t\tPredicted Word-text:  ['increase']\n",
      "\n",
      "\n",
      "5.Input Sequence-tokens:  [ 0  0  0  0  0  0  0  0  6 99]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
      "\t\tPredicted Word-token:  [1]\n",
      "5.Input Sequence-text:  ['that thereby']\n",
      "\tInput Word-text:  ['that']\n",
      "\t\tPredicted Word-text:  ['thou']\n",
      "\n",
      "\n",
      "6.Input Sequence-tokens:  [ 0  0  0  0  0  0  0  6 99 27]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 0, 0, 0, 6, 99]\n",
      "\t\tPredicted Word-token:  [27]\n",
      "6.Input Sequence-text:  [\"that thereby beauty's\"]\n",
      "\tInput Word-text:  ['that thereby']\n",
      "\t\tPredicted Word-text:  [\"beauty's\"]\n",
      "\n",
      "\n",
      "7.Input Sequence-tokens:  [  0   0   0   0   0   0   6  99  27 192]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 0, 0, 6, 99, 27]\n",
      "\t\tPredicted Word-token:  [192]\n",
      "7.Input Sequence-text:  [\"that thereby beauty's rose\"]\n",
      "\tInput Word-text:  [\"that thereby beauty's\"]\n",
      "\t\tPredicted Word-text:  ['rose']\n",
      "\n",
      "\n",
      "8.Input Sequence-tokens:  [  0   0   0   0   0   6  99  27 192 100]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 0, 6, 99, 27, 192]\n",
      "\t\tPredicted Word-token:  [100]\n",
      "8.Input Sequence-text:  [\"that thereby beauty's rose might\"]\n",
      "\tInput Word-text:  [\"that thereby beauty's rose\"]\n",
      "\t\tPredicted Word-text:  ['might']\n",
      "\n",
      "\n",
      "9.Input Sequence-tokens:  [  0   0   0   0   6  99  27 192 100 101]\n",
      "\tInput Word-token:  [0, 0, 0, 0, 6, 99, 27, 192, 100]\n",
      "\t\tPredicted Word-token:  [101]\n",
      "9.Input Sequence-text:  [\"that thereby beauty's rose might never\"]\n",
      "\tInput Word-text:  [\"that thereby beauty's rose might\"]\n",
      "\t\tPredicted Word-text:  ['never']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(sequences)):\n",
    "    if idx > 9:\n",
    "        break\n",
    "    \n",
    "    input_sequence = sequences[idx,:]\n",
    "\n",
    "    #input_words_tokens = np.insert(input_sequence[:-1],0,0)\n",
    "    input_words_tokens = input_sequence[:-1].tolist()\n",
    "\n",
    "    y_pred = model.predict([input_words_tokens], verbose=0)\n",
    "    predicted_token = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    print(f\"{idx}.Input Sequence-tokens: \", input_sequence)\n",
    "    print(f\"\\tInput Word-token: \", input_words_tokens)\n",
    "    print(f\"\\t\\tPredicted Word-token: \", predicted_token)\n",
    "\n",
    "    print(f\"{idx}.Input Sequence-text: \", tokenizer.sequences_to_texts([input_sequence]))\n",
    "    print(f\"\\tInput Word-text: \", tokenizer.sequences_to_texts([input_words_tokens]))\n",
    "    print(f\"\\t\\tPredicted Word-text: \", tokenizer.sequences_to_texts([predicted_token]))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11528e-1b29-47ef-9b08-0a975466c92f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
